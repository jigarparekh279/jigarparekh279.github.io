---
layout: default
modal-id: 6
date: 2019-11-30
img: gradient_decent.png
alt: image-alt
project-date: November 2019
link: https://github.com/Franjcf/Data-Science-Projects/blob/main/gradient_decent_optimization_and_implementation/unconstrained_optimization_with_gradient_decent.ipynb
TLDR: Gradient decent optimization works best when using variable step sizes that are dictated by the eigenvalues of Hessian data matrix.
skills: Vector Calculus, Linear Algebra, Optimization, Gradient Decent, Python, Ridge and Linear Regression.
description: 'In this mini-project I code gradient decent from scratch to solve linear regression and ridge regularization problems. These particular problems were chosen because their analytical solutions are well-known. Furthermore I investigate how the gradient step size affects the rate of convergence of the underlying optimization problem. I then proceed by calculating the largest and smallest eigenvalues of the second derivative of objective function in order to set optimal step size and to find the lower bound the rate of convergence. Finally I investigate how the regularization term (lambda) affects said convergence rate.'
---
